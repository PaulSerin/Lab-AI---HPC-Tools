slurmstepd: info: Setting TMPDIR to /scratch/8692773. Previous errors about TMPDIR can be discarded
/var/spool/slurmd/job8692773/slurm_script: ligne 10: /mnt/netapp2/Store_uni//home/ulc/cursos/curso363/mypython/bin/deactivate: No such file or directory
/mnt/netapp2/Store_uni/home/ulc/cursos/curso363/mypython/bin/python
/mnt/netapp2/Store_uni/home/ulc/cursos/curso363/mypython/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/mnt/netapp2/Store_uni/home/ulc/cursos/curso363/mypython/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Using GPU: NVIDIA A100-PCIE-40GB
GPU Device Index: 0
Number of GPUs: 1

                        ##################################################
                        #                                                #
                        #                 START TRAINING                 #
                        #                                                #
                        ##################################################


############ Train Epoch 1 ############
Batch 100 / 10853, Loss: 3.2
Batch 200 / 10853, Loss: 3.4
Batch 300 / 10853, Loss: 2.0
Batch 400 / 10853, Loss: 2.5
Batch 500 / 10853, Loss: 2.9
Batch 600 / 10853, Loss: 2.7
Batch 700 / 10853, Loss: 1.6
Batch 800 / 10853, Loss: 2.1
Batch 900 / 10853, Loss: 2.3
Batch 1000 / 10853, Loss: 1.1
Batch 1100 / 10853, Loss: 2.2
Batch 1200 / 10853, Loss: 0.8
Batch 1300 / 10853, Loss: 0.9
Batch 1400 / 10853, Loss: 0.8
Batch 1500 / 10853, Loss: 1.9
Batch 1600 / 10853, Loss: 1.4
Batch 1700 / 10853, Loss: 1.1
Batch 1800 / 10853, Loss: 1.3
Batch 1900 / 10853, Loss: 1.8
Batch 2000 / 10853, Loss: 1.2
Batch 2100 / 10853, Loss: 2.5
Batch 2200 / 10853, Loss: 2.4
Batch 2300 / 10853, Loss: 1.3
Batch 2400 / 10853, Loss: 1.5
Batch 2500 / 10853, Loss: 1.4
Batch 2600 / 10853, Loss: 1.0
Batch 2700 / 10853, Loss: 1.4
Batch 2800 / 10853, Loss: 0.9
Batch 2900 / 10853, Loss: 0.6
Batch 3000 / 10853, Loss: 1.9
Batch 3100 / 10853, Loss: 1.1
Batch 3200 / 10853, Loss: 1.5
Batch 3300 / 10853, Loss: 1.2
Batch 3400 / 10853, Loss: 1.7
Batch 3500 / 10853, Loss: 0.8
Batch 3600 / 10853, Loss: 0.7
Batch 3700 / 10853, Loss: 1.6
Batch 3800 / 10853, Loss: 1.3
Batch 3900 / 10853, Loss: 1.9
Batch 4000 / 10853, Loss: 2.1
Batch 4100 / 10853, Loss: 1.2
Batch 4200 / 10853, Loss: 1.7
Batch 4300 / 10853, Loss: 0.6
Batch 4400 / 10853, Loss: 0.9
Batch 4500 / 10853, Loss: 1.1
Batch 4600 / 10853, Loss: 1.2
Batch 4700 / 10853, Loss: 0.4
Batch 4800 / 10853, Loss: 0.8
Batch 4900 / 10853, Loss: 1.6
Batch 5000 / 10853, Loss: 1.1
Batch 5100 / 10853, Loss: 0.8
Batch 5200 / 10853, Loss: 0.3
Batch 5300 / 10853, Loss: 0.5
Batch 5400 / 10853, Loss: 1.0
Batch 5500 / 10853, Loss: 1.3
Batch 5600 / 10853, Loss: 1.2
Batch 5700 / 10853, Loss: 0.9
Batch 5800 / 10853, Loss: 1.9
Batch 5900 / 10853, Loss: 0.9
Batch 6000 / 10853, Loss: 0.8
Batch 6100 / 10853, Loss: 0.7
Batch 6200 / 10853, Loss: 1.5
Batch 6300 / 10853, Loss: 1.4
Batch 6400 / 10853, Loss: 0.4
Batch 6500 / 10853, Loss: 0.6
Batch 6600 / 10853, Loss: 0.5
Batch 6700 / 10853, Loss: 1.3
Batch 6800 / 10853, Loss: 0.8
Batch 6900 / 10853, Loss: 0.9
Batch 7000 / 10853, Loss: 1.5
Batch 7100 / 10853, Loss: 1.1
Batch 7200 / 10853, Loss: 1.1
Batch 7300 / 10853, Loss: 0.9
Batch 7400 / 10853, Loss: 1.3
Batch 7500 / 10853, Loss: 0.9
Batch 7600 / 10853, Loss: 1.4
Batch 7700 / 10853, Loss: 1.7
Batch 7800 / 10853, Loss: 1.4
Batch 7900 / 10853, Loss: 2.1
Batch 8000 / 10853, Loss: 1.5
Batch 8100 / 10853, Loss: 0.6
Batch 8200 / 10853, Loss: 0.6
Batch 8300 / 10853, Loss: 1.1
Batch 8400 / 10853, Loss: 0.7
Batch 8500 / 10853, Loss: 1.2
Batch 8600 / 10853, Loss: 0.9
Batch 8700 / 10853, Loss: 1.5
Batch 8800 / 10853, Loss: 0.6
Batch 8900 / 10853, Loss: 0.5
Batch 9000 / 10853, Loss: 1.2
Batch 9100 / 10853, Loss: 1.3
Batch 9200 / 10853, Loss: 1.5
Batch 9300 / 10853, Loss: 0.9
Batch 9400 / 10853, Loss: 1.6
Batch 9500 / 10853, Loss: 0.5
Batch 9600 / 10853, Loss: 1.2
Batch 9700 / 10853, Loss: 1.5
Batch 9800 / 10853, Loss: 1.6
Batch 9900 / 10853, Loss: 1.5
Batch 10000 / 10853, Loss: 0.8
Batch 10100 / 10853, Loss: 1.7
Batch 10200 / 10853, Loss: 1.1
Batch 10300 / 10853, Loss: 1.2
Batch 10400 / 10853, Loss: 0.4
Batch 10500 / 10853, Loss: 0.9
Batch 10600 / 10853, Loss: 1.1
Batch 10700 / 10853, Loss: 0.8
Batch 10800 / 10853, Loss: 0.8
Epoch 1 finished. Training Loss: 1.3411627696767157, Time: 1890.1427474021912


############ Train Epoch 2 ############
Batch 100 / 10853, Loss: 0.9
Batch 200 / 10853, Loss: 1.2
Batch 300 / 10853, Loss: 0.9
Batch 400 / 10853, Loss: 0.9
Batch 500 / 10853, Loss: 1.0
Batch 600 / 10853, Loss: 0.8
Batch 700 / 10853, Loss: 0.6
Batch 800 / 10853, Loss: 0.4
Batch 900 / 10853, Loss: 1.2
Batch 1000 / 10853, Loss: 1.5
Batch 1100 / 10853, Loss: 0.1
Batch 1200 / 10853, Loss: 1.1
Batch 1300 / 10853, Loss: 0.9
Batch 1400 / 10853, Loss: 0.6
Batch 1500 / 10853, Loss: 1.0
Batch 1600 / 10853, Loss: 0.6
Batch 1700 / 10853, Loss: 1.1
Batch 1800 / 10853, Loss: 0.9
Batch 1900 / 10853, Loss: 0.8
Batch 2000 / 10853, Loss: 0.3
Batch 2100 / 10853, Loss: 0.9
Batch 2200 / 10853, Loss: 0.9
Batch 2300 / 10853, Loss: 1.1
Batch 2400 / 10853, Loss: 0.9
Batch 2500 / 10853, Loss: 0.4
Batch 2600 / 10853, Loss: 1.1
Batch 2700 / 10853, Loss: 1.2
Batch 2800 / 10853, Loss: 0.6
Batch 2900 / 10853, Loss: 0.3
Batch 3000 / 10853, Loss: 0.4
Batch 3100 / 10853, Loss: 1.2
Batch 3200 / 10853, Loss: 1.0
Batch 3300 / 10853, Loss: 0.2
Batch 3400 / 10853, Loss: 0.5
Batch 3500 / 10853, Loss: 0.3
Batch 3600 / 10853, Loss: 0.8
Batch 3700 / 10853, Loss: 0.4
Batch 3800 / 10853, Loss: 0.2
Batch 3900 / 10853, Loss: 0.7
Batch 4000 / 10853, Loss: 1.0
Batch 4100 / 10853, Loss: 1.6
Batch 4200 / 10853, Loss: 1.4
Batch 4300 / 10853, Loss: 1.4
Batch 4400 / 10853, Loss: 1.1
Batch 4500 / 10853, Loss: 0.9
Batch 4600 / 10853, Loss: 0.8
Batch 4700 / 10853, Loss: 0.5
Batch 4800 / 10853, Loss: 1.1
Batch 4900 / 10853, Loss: 0.7
Batch 5000 / 10853, Loss: 0.6
Batch 5100 / 10853, Loss: 0.7
Batch 5200 / 10853, Loss: 0.5
Batch 5300 / 10853, Loss: 0.8
Batch 5400 / 10853, Loss: 1.0
Batch 5500 / 10853, Loss: 1.8
Batch 5600 / 10853, Loss: 1.0
Batch 5700 / 10853, Loss: 1.1
Batch 5800 / 10853, Loss: 1.2
Batch 5900 / 10853, Loss: 2.1
Batch 6000 / 10853, Loss: 0.6
Batch 6100 / 10853, Loss: 0.7
Batch 6200 / 10853, Loss: 1.0
Batch 6300 / 10853, Loss: 0.9
Batch 6400 / 10853, Loss: 0.8
Batch 6500 / 10853, Loss: 1.4
Batch 6600 / 10853, Loss: 0.7
Batch 6700 / 10853, Loss: 1.1
Batch 6800 / 10853, Loss: 1.9
Batch 6900 / 10853, Loss: 1.4
Batch 7000 / 10853, Loss: 0.5
Batch 7100 / 10853, Loss: 0.5
Batch 7200 / 10853, Loss: 0.8
Batch 7300 / 10853, Loss: 0.3
Batch 7400 / 10853, Loss: 0.8
Batch 7500 / 10853, Loss: 0.7
Batch 7600 / 10853, Loss: 1.0
Batch 7700 / 10853, Loss: 0.8
Batch 7800 / 10853, Loss: 0.7
Batch 7900 / 10853, Loss: 0.8
Batch 8000 / 10853, Loss: 0.4
Batch 8100 / 10853, Loss: 1.7
Batch 8200 / 10853, Loss: 0.7
Batch 8300 / 10853, Loss: 0.7
Batch 8400 / 10853, Loss: 1.2
Batch 8500 / 10853, Loss: 1.2
Batch 8600 / 10853, Loss: 0.6
Batch 8700 / 10853, Loss: 0.4
Batch 8800 / 10853, Loss: 0.8
Batch 8900 / 10853, Loss: 1.9
Batch 9000 / 10853, Loss: 0.7
Batch 9100 / 10853, Loss: 1.4
Batch 9200 / 10853, Loss: 0.7
Batch 9300 / 10853, Loss: 0.4
Batch 9400 / 10853, Loss: 1.0
Batch 9500 / 10853, Loss: 1.9
Batch 9600 / 10853, Loss: 0.6
Batch 9700 / 10853, Loss: 0.5
Batch 9800 / 10853, Loss: 1.0
Batch 9900 / 10853, Loss: 0.3
Batch 10000 / 10853, Loss: 1.0
Batch 10100 / 10853, Loss: 0.7
Batch 10200 / 10853, Loss: 0.5
Batch 10300 / 10853, Loss: 0.4
Batch 10400 / 10853, Loss: 1.1
Batch 10500 / 10853, Loss: 0.6
Batch 10600 / 10853, Loss: 0.7
Batch 10700 / 10853, Loss: 1.0
Batch 10800 / 10853, Loss: 1.3
Epoch 2 finished. Training Loss: 0.886872178740785, Time: 1891.624856710434

Total Training Time: 3781.77 seconds

                        ##################################################
                        #                                                #
                        #                      END                       #
                        #                                                #
                        ##################################################

